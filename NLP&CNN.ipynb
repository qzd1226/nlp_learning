{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu', strides = 1, input_shape = (1000, 300)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于词之间的相对垂直关系是任意的，因此关联信息主要体现在水平方向上。 所以卷积使用的卷积核为扁平的一维卷积核。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cd introduction_to_ml_with_python/data\n",
    "wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "tar xvzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "def pre_process_data(filepath):\n",
    "    \"\"\"\n",
    "    This is dependent on your training data source but we will try to generalize it as best as possible.\n",
    "    \"\"\"\n",
    "    positive_path = os.path.join(filepath, 'pos')\n",
    "    negative_path = os.path.join(filepath, 'neg')\n",
    "\n",
    "    pos_label = 1\n",
    "    neg_label = 0\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    for filename in glob.glob(\"H:/NLP_Data/aclImdb_v1/aclImdb/train/pos/*.txt\"):\n",
    "        with open(filename, 'r', encoding = 'utf-8', errors='ignore' 'r') as f:\n",
    "            dataset.append((pos_label, f.read()))\n",
    "\n",
    "    for filename in glob.glob(\"H:/NLP_Data/aclImdb_v1/aclImdb/train/neg/*.txt\"):\n",
    "        with open(filename, 'r', encoding = 'utf-8', errors='ignore') as f:\n",
    "            dataset.append((neg_label, f.read()))\n",
    "\n",
    "    shuffle(dataset)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pre_process_data('H:/NLP_Data/aclImdb_v1/aclImdb/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " \"To all the miserable people who have done everything from complain about the dialogue, the budget, the this and the that....who wants to hear it? IF you missed the point of this beyond-beautiful movie, that's your loss. The rest of us who deeply love this movie do not care what you think. I am a thirthysomething guy who has seen thousands of movies in my life, and this one stands in its own entity, in my book. It was not supposed to be a documentary, or a completely factual account of what happened that night. It is the most amazing love story ever attempted. I know that it is the cynical 90's and the millennium has everyone in a tizzy, but come on. Someone on this comments board complained that it made too much money! How lame is that? It made bundles of money in every civilized country on the planet, and is the top grossing film in the planet. I will gladly side with the majority this time around. Okay, cynics, time to crawl back under your rock, I am done.\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\ANACONDA\\lib\\site-packages\\pugnlp\\constants.py:136: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  [datetime.datetime, pd.datetime, pd.Timestamp])\n",
      "H:\\ANACONDA\\lib\\site-packages\\pugnlp\\constants.py:158: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n",
      "H:\\ANACONDA\\lib\\site-packages\\pugnlp\\tutil.py:100: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "H:\\ANACONDA\\lib\\site-packages\\pugnlp\\util.py:80: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "INFO:nlpia.constants:Starting logger in nlpia.constants...\n",
      "H:\\ANACONDA\\lib\\site-packages\\nlpia\\futil.py:30: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "H:\\ANACONDA\\lib\\site-packages\\nlpia\\loaders.py:78: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "INFO:nlpia.loaders:No BIGDATA index found in H:\\ANACONDA\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv so copy H:\\ANACONDA\\lib\\site-packages\\nlpia\\data\\bigdata_info.latest.csv to H:\\ANACONDA\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv if you want to \"freeze\" it.\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('H:\\\\ANACONDA\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\mavis-batey-greetings.csv',), **{'low_memory': False})`...\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('H:\\\\ANACONDA\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\sms-spam.csv',), **{'low_memory': False})`...\n",
      "INFO:nlpia.loaders:Downloading w2v\n",
      "INFO:nlpia.web:URL too short: w2v\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.300d\\.zip$, string=googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.27b\\.zip$, string=googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.42b\\.zip$, string=googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.6b\\.zip$, string=googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.bin\\.gz$, string=googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.tgz$, string=googlenews-vectors-negative300.bin.gz\n",
      "INFO:nlpia.loaders:expanded+normalized file path: H:\\ANACONDA\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "INFO:nlpia.loaders:requesting URL: https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1\n",
      "INFO:nlpia.loaders:remote_size: 1647046227\n",
      "INFO:nlpia.loaders:local_size: 1647046227\n",
      "INFO:nlpia.loaders:retained: H:\\ANACONDA\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "WARNING:nlpia.loaders:normalize_ext.filepath=H:\\ANACONDA\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.300d\\.zip$, string=H:\\ANACONDA\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.27b\\.zip$, string=H:\\ANACONDA\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.42b\\.zip$, string=H:\\ANACONDA\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.6b\\.zip$, string=H:\\ANACONDA\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.bin\\.gz$, string=H:\\ANACONDA\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.tgz$, string=H:\\ANACONDA\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "WARNING:nlpia.loaders:download_unzip.new_filepaths=H:\\ANACONDA\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.loaders:nlpia.loaders.rename_file(source=H:\\ANACONDA\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz, dest=H:\\ANACONDA\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz)\n",
      "DEBUG:nlpia.loaders:nlpia.loaders.os.rename(source=H:\\ANACONDA\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz, dest=H:\\ANACONDA\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz)\n",
      "WARNING:nlpia.loaders:download_unzip.filepath=H:\\ANACONDA\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.loaders:downloaded name=w2v to filepath=H:\\ANACONDA\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.loaders:nlpia.loaders.get_data.filepaths={'w2v': 'H:\\\\ANACONDA\\\\lib\\\\site-packages\\\\nlpia\\\\bigdata\\\\googlenews-vectors-negative300.bin.gz'}\n",
      "DEBUG:nlpia.loaders:nlpia.loaders.get_data.filepath=H:\\ANACONDA\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "INFO:gensim.models.utils_any2vec:loading projection weights from H:\\ANACONDA\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "INFO:gensim.models.utils_any2vec:loaded (3000000, 300) matrix from H:\\ANACONDA\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n"
     ]
    }
   ],
   "source": [
    "from nlpia.loaders import get_data\n",
    "word_vectors = get_data('w2v', limit = 200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_vectorize(dataset):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenizer.tokenize(sample[1])\n",
    "        sample_vecs = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sample_vecs.append(word_vectors[token])\n",
    "\n",
    "            except KeyError:\n",
    "                pass  # No matching token in the Google w2v vocab\n",
    "\n",
    "        vectorized_data.append(sample_vecs)\n",
    "\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_expected(dataset):\n",
    "    \"\"\" Peel of the target values from the dataset \"\"\"\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        expected.append(sample[0])\n",
    "    return expected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = tokenize_and_vectorize(dataset)\n",
    "expected = collect_expected(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = int(len(vectorized_data) * .8)\n",
    "\n",
    "x_train = vectorized_data[:split_point]\n",
    "y_train = expected[:split_point]\n",
    "x_test = vectorized_data[split_point:]\n",
    "y_test = expected[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "maxlen = 400\n",
    "batch_size = 32         # How many samples to show the net before backpropogating the error and updating the weights\n",
    "embedding_dims = 300    # Length of the token vectors we will create for passing into the Convnet\n",
    "filters = 250           # Number of filters we will train\n",
    "kernel_size = 3         # The width of the filters, actual filters will each be a matrix of weights of size: embedding_dims x kernel_size or 50 x 3 in our case\n",
    "hidden_dims = 250       # Number of neurons in the plain feed forward net at the end of the chain\n",
    "epochs = 2              # Number of times we will pass the entire training dataset through the network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN网络输入尺寸需要一致，但是每个文本的向量大小却不同 因此需要对长的截取短的补0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trunc(data, maxlen):\n",
    "    \"\"\" For a given dataset pad with zero vectors or truncate to maxlen \"\"\"\n",
    "    new_data = []\n",
    "\n",
    "    # Create a vector of 0's the length of our word vectors\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "\n",
    "    for sample in data:\n",
    "\n",
    "        if len(sample) > maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample\n",
    "            additional_elems = maxlen - len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "    return new_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_trunc(x_train, maxlen)\n",
    "x_test = pad_trunc(x_test, maxlen)\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
    "y_test = np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练CNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 441s 696ms/step - loss: 0.4858 - accuracy: 0.7488 - val_loss: 0.4490 - val_accuracy: 0.7930\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 60s 95ms/step - loss: 0.2284 - accuracy: 0.9054 - val_loss: 0.3268 - val_accuracy: 0.8612\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1,\n",
    "                 input_shape=(maxlen, embedding_dims)))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "model_structure = model.to_json()\n",
    "with open(\"cnn_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "\n",
    "model.save_weights(\"cnn_weights.h5\")\n",
    "print('Model saved.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
