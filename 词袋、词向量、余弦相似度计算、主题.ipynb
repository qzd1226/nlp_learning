{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词袋\n",
    "词出现的频率或者词频向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The faster Harry got to the store, the faster Harry, the faster, would get home\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " 'got',\n",
       " 'to',\n",
       " 'the',\n",
       " 'store',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " ',',\n",
       " 'would',\n",
       " 'get',\n",
       " 'home']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence.lower())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python的字典可以帮助我们获得独立的词及其出现的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 4,\n",
       "         'faster': 3,\n",
       "         'harry': 2,\n",
       "         'got': 1,\n",
       "         'to': 1,\n",
       "         'store': 1,\n",
       "         ',': 3,\n",
       "         'would': 1,\n",
       "         'get': 1,\n",
       "         'home': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "bag_of_words = Counter(tokens)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4), ('faster', 3), (',', 3), ('harry', 2)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.most_common(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_harry_appears = bag_of_words['harry']\n",
    "num_unique_words = len(bag_of_words)\n",
    "tf = times_harry_appears/num_unique_words\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kite_text = \"A kite is traditionally a tethered heavier-than-air craft with wing surfaces that react against the air to create lift and drag. A kite consists of wings, tethers, and anchors. Kites often have a bridle to guide the face of the kite at the correct angle so the wind can lift it. A kite's wing also may be so designed so a bridle is not needed; when kiting a sailplane for launch, the tether meets the wing at a single point. A kite may have fixed or moving anchors. Untraditionally in technical kiting, a kite consists of tether-set-coupled wing sets; even in technical kiting, though, a wing in the system is still often called the kite. The lift that sustains the kite in flight is generated when air flows around the kite's surface, producing low pressure above and high pressure below the wings. The interaction with the wind also generates horizontal drag along the direction of the wind. The resultant force vector from the lift and drag force components is opposed by the tension of one or more of the lines or tethers to which the kite is attached. The anchor point of the kite line may be static or moving (e.g., the towing of a kite by a running person, boat, free-falling anchors as in paragliders and fugitive parakites or vehicle). The same principles of fluid flow apply in liquids and kites are also used under water. A hybrid tethered craft comprising both a lighter-than-air balloon as well as a kite lifting surface is called a kytoon. Kites have a long and varied history and many different types are flown individually and at festivals worldwide. Kites may be flown for recreation, art or other practical uses. Sport kites can be flown in aerial ballet, sometimes as part of a competition. Power kites are multi-line steerable kites designed to generate large forces which can be used to power activities such as kite surfing, kite landboarding, kite fishing, kite buggying and a new trend snow kiting. Even Man-lifting kites have been made.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 26, 'a': 20, 'kite': 16, ',': 15, 'and': 10, 'of': 10, 'kites': 8, 'is': 7, 'in': 7, 'or': 6, 'wing': 5, 'to': 5, 'be': 5, 'as': 5, 'lift': 4, 'have': 4, 'may': 4, 'at': 3, 'so': 3, 'can': 3, 'also': 3, 'kiting': 3, 'are': 3, 'flown': 3, 'tethered': 2, 'craft': 2, 'with': 2, 'that': 2, 'air': 2, 'consists': 2, 'tethers': 2, 'anchors.': 2, 'often': 2, 'bridle': 2, 'wind': 2, \"'s\": 2, 'designed': 2, ';': 2, 'when': 2, 'for': 2, 'moving': 2, 'technical': 2, 'even': 2, 'called': 2, 'surface': 2, 'pressure': 2, 'drag': 2, 'force': 2, 'by': 2, 'which': 2, '.': 2, 'used': 2, 'power': 2, 'traditionally': 1, 'heavier-than-air': 1, 'surfaces': 1, 'react': 1, 'against': 1, 'create': 1, 'drag.': 1, 'wings': 1, 'guide': 1, 'face': 1, 'correct': 1, 'angle': 1, 'it.': 1, 'not': 1, 'needed': 1, 'sailplane': 1, 'launch': 1, 'tether': 1, 'meets': 1, 'single': 1, 'point.': 1, 'fixed': 1, 'untraditionally': 1, 'tether-set-coupled': 1, 'sets': 1, 'though': 1, 'system': 1, 'still': 1, 'kite.': 1, 'sustains': 1, 'flight': 1, 'generated': 1, 'flows': 1, 'around': 1, 'producing': 1, 'low': 1, 'above': 1, 'high': 1, 'below': 1, 'wings.': 1, 'interaction': 1, 'generates': 1, 'horizontal': 1, 'along': 1, 'direction': 1, 'wind.': 1, 'resultant': 1, 'vector': 1, 'from': 1, 'components': 1, 'opposed': 1, 'tension': 1, 'one': 1, 'more': 1, 'lines': 1, 'attached.': 1, 'anchor': 1, 'point': 1, 'line': 1, 'static': 1, '(': 1, 'e.g.': 1, 'towing': 1, 'running': 1, 'person': 1, 'boat': 1, 'free-falling': 1, 'anchors': 1, 'paragliders': 1, 'fugitive': 1, 'parakites': 1, 'vehicle': 1, ')': 1, 'same': 1, 'principles': 1, 'fluid': 1, 'flow': 1, 'apply': 1, 'liquids': 1, 'under': 1, 'water.': 1, 'hybrid': 1, 'comprising': 1, 'both': 1, 'lighter-than-air': 1, 'balloon': 1, 'well': 1, 'lifting': 1, 'kytoon.': 1, 'long': 1, 'varied': 1, 'history': 1, 'many': 1, 'different': 1, 'types': 1, 'individually': 1, 'festivals': 1, 'worldwide.': 1, 'recreation': 1, 'art': 1, 'other': 1, 'practical': 1, 'uses.': 1, 'sport': 1, 'aerial': 1, 'ballet': 1, 'sometimes': 1, 'part': 1, 'competition.': 1, 'multi-line': 1, 'steerable': 1, 'generate': 1, 'large': 1, 'forces': 1, 'activities': 1, 'such': 1, 'surfing': 1, 'landboarding': 1, 'fishing': 1, 'buggying': 1, 'new': 1, 'trend': 1, 'snow': 1, 'kiting.': 1, 'man-lifting': 1, 'been': 1, 'made': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# kite_text = \"A kite is traditionally ...\"  # Step left to user, so we aren't repeating ourselves\n",
    "tokens = tokenizer.tokenize(kite_text.lower())\n",
    "token_sequence = Counter(tokens)\n",
    "print(token_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\69016\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'kite': 16,\n",
       "         'traditionally': 1,\n",
       "         'tethered': 2,\n",
       "         'heavier-than-air': 1,\n",
       "         'craft': 2,\n",
       "         'wing': 5,\n",
       "         'surfaces': 1,\n",
       "         'react': 1,\n",
       "         'air': 2,\n",
       "         'create': 1,\n",
       "         'lift': 4,\n",
       "         'drag.': 1,\n",
       "         'consists': 2,\n",
       "         'wings': 1,\n",
       "         ',': 15,\n",
       "         'tethers': 2,\n",
       "         'anchors.': 2,\n",
       "         'kites': 8,\n",
       "         'often': 2,\n",
       "         'bridle': 2,\n",
       "         'guide': 1,\n",
       "         'face': 1,\n",
       "         'correct': 1,\n",
       "         'angle': 1,\n",
       "         'wind': 2,\n",
       "         'it.': 1,\n",
       "         \"'s\": 2,\n",
       "         'also': 3,\n",
       "         'may': 4,\n",
       "         'designed': 2,\n",
       "         'needed': 1,\n",
       "         ';': 2,\n",
       "         'kiting': 3,\n",
       "         'sailplane': 1,\n",
       "         'launch': 1,\n",
       "         'tether': 1,\n",
       "         'meets': 1,\n",
       "         'single': 1,\n",
       "         'point.': 1,\n",
       "         'fixed': 1,\n",
       "         'moving': 2,\n",
       "         'untraditionally': 1,\n",
       "         'technical': 2,\n",
       "         'tether-set-coupled': 1,\n",
       "         'sets': 1,\n",
       "         'even': 2,\n",
       "         'though': 1,\n",
       "         'system': 1,\n",
       "         'still': 1,\n",
       "         'called': 2,\n",
       "         'kite.': 1,\n",
       "         'sustains': 1,\n",
       "         'flight': 1,\n",
       "         'generated': 1,\n",
       "         'flows': 1,\n",
       "         'around': 1,\n",
       "         'surface': 2,\n",
       "         'producing': 1,\n",
       "         'low': 1,\n",
       "         'pressure': 2,\n",
       "         'high': 1,\n",
       "         'wings.': 1,\n",
       "         'interaction': 1,\n",
       "         'generates': 1,\n",
       "         'horizontal': 1,\n",
       "         'drag': 2,\n",
       "         'along': 1,\n",
       "         'direction': 1,\n",
       "         'wind.': 1,\n",
       "         'resultant': 1,\n",
       "         'force': 2,\n",
       "         'vector': 1,\n",
       "         'components': 1,\n",
       "         'opposed': 1,\n",
       "         'tension': 1,\n",
       "         'one': 1,\n",
       "         'lines': 1,\n",
       "         'attached.': 1,\n",
       "         'anchor': 1,\n",
       "         'point': 1,\n",
       "         'line': 1,\n",
       "         'static': 1,\n",
       "         '(': 1,\n",
       "         'e.g.': 1,\n",
       "         'towing': 1,\n",
       "         'running': 1,\n",
       "         'person': 1,\n",
       "         'boat': 1,\n",
       "         'free-falling': 1,\n",
       "         'anchors': 1,\n",
       "         'paragliders': 1,\n",
       "         'fugitive': 1,\n",
       "         'parakites': 1,\n",
       "         'vehicle': 1,\n",
       "         ')': 1,\n",
       "         '.': 2,\n",
       "         'principles': 1,\n",
       "         'fluid': 1,\n",
       "         'flow': 1,\n",
       "         'apply': 1,\n",
       "         'liquids': 1,\n",
       "         'used': 2,\n",
       "         'water.': 1,\n",
       "         'hybrid': 1,\n",
       "         'comprising': 1,\n",
       "         'lighter-than-air': 1,\n",
       "         'balloon': 1,\n",
       "         'well': 1,\n",
       "         'lifting': 1,\n",
       "         'kytoon.': 1,\n",
       "         'long': 1,\n",
       "         'varied': 1,\n",
       "         'history': 1,\n",
       "         'many': 1,\n",
       "         'different': 1,\n",
       "         'types': 1,\n",
       "         'flown': 3,\n",
       "         'individually': 1,\n",
       "         'festivals': 1,\n",
       "         'worldwide.': 1,\n",
       "         'recreation': 1,\n",
       "         'art': 1,\n",
       "         'practical': 1,\n",
       "         'uses.': 1,\n",
       "         'sport': 1,\n",
       "         'aerial': 1,\n",
       "         'ballet': 1,\n",
       "         'sometimes': 1,\n",
       "         'part': 1,\n",
       "         'competition.': 1,\n",
       "         'power': 2,\n",
       "         'multi-line': 1,\n",
       "         'steerable': 1,\n",
       "         'generate': 1,\n",
       "         'large': 1,\n",
       "         'forces': 1,\n",
       "         'activities': 1,\n",
       "         'surfing': 1,\n",
       "         'landboarding': 1,\n",
       "         'fishing': 1,\n",
       "         'buggying': 1,\n",
       "         'new': 1,\n",
       "         'trend': 1,\n",
       "         'snow': 1,\n",
       "         'kiting.': 1,\n",
       "         'man-lifting': 1,\n",
       "         'made': 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "tokens = [x for x in tokens if x not in stopwords]\n",
    "kite_count = Counter(tokens)\n",
    "kite_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 向量化\n",
    "使用词频来描述一个字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07207207207207207,\n",
       " 0.06756756756756757,\n",
       " 0.036036036036036036,\n",
       " 0.02252252252252252,\n",
       " 0.018018018018018018,\n",
       " 0.018018018018018018,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vector = []\n",
    "doc_length = len(tokens)\n",
    "for key, value in kite_count.most_common():\n",
    "    document_vector.append(value / doc_length)\n",
    "\n",
    "document_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_0 = \"The faster Harry got to the store, the faster Harry, the faster, would get home.\"\n",
    "doc_1 = \"Harry is hairy and faster than Jill.\"\n",
    "doc_2 = \"Jill is not as hairy as Harry.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_0 = tokenizer.tokenize(doc_0.lower())\n",
    "tokens_1 = tokenizer.tokenize(doc_1.lower())\n",
    "tokens_2 = tokenizer.tokenize(doc_2.lower())\n",
    "lexicon = set(tokens_0 + tokens_1 + tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " 'got',\n",
       " 'to',\n",
       " 'the',\n",
       " 'store',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " ',',\n",
       " 'would',\n",
       " 'get',\n",
       " 'home',\n",
       " '.',\n",
       " 'harry',\n",
       " 'is',\n",
       " 'hairy',\n",
       " 'and',\n",
       " 'faster',\n",
       " 'than',\n",
       " 'jill',\n",
       " '.',\n",
       " 'jill',\n",
       " 'is',\n",
       " 'not',\n",
       " 'as',\n",
       " 'hairy',\n",
       " 'as',\n",
       " 'harry',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tokens_0 + tokens_1 + tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',',\n",
       " '.',\n",
       " 'and',\n",
       " 'as',\n",
       " 'faster',\n",
       " 'get',\n",
       " 'got',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'home',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'not',\n",
       " 'store',\n",
       " 'than',\n",
       " 'the',\n",
       " 'to',\n",
       " 'would'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('to', 0), (',', 0), ('got', 0), ('get', 0), ('than', 0), ('.', 0), ('faster', 0), ('store', 0), ('as', 0), ('the', 0), ('would', 0), ('hairy', 0), ('harry', 0), ('is', 0), ('jill', 0), ('and', 0), ('not', 0), ('home', 0)])\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "vector_template = OrderedDict((token, 0) for token in lexicon)\n",
    "print(vector_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "document_vectors = []\n",
    "for doc in [doc_0, doc_1, doc_2]:\n",
    "\n",
    "    vec = copy.copy(vector_template)  # So we are dealing with new objects, not multiple references to the same object\n",
    "\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "\n",
    "    for key, value in token_counts.items():\n",
    "        vec[key] = value / len(lexicon)\n",
    "    document_vectors.append(vec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('to', 0.05555555555555555),\n",
       "              (',', 0.16666666666666666),\n",
       "              ('got', 0.05555555555555555),\n",
       "              ('get', 0.05555555555555555),\n",
       "              ('than', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('faster', 0.16666666666666666),\n",
       "              ('store', 0.05555555555555555),\n",
       "              ('as', 0),\n",
       "              ('the', 0.2222222222222222),\n",
       "              ('would', 0.05555555555555555),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.1111111111111111),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('and', 0),\n",
       "              ('not', 0),\n",
       "              ('home', 0.05555555555555555)]),\n",
       " OrderedDict([('to', 0),\n",
       "              (',', 0),\n",
       "              ('got', 0),\n",
       "              ('get', 0),\n",
       "              ('than', 0.05555555555555555),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('faster', 0.05555555555555555),\n",
       "              ('store', 0),\n",
       "              ('as', 0),\n",
       "              ('the', 0),\n",
       "              ('would', 0),\n",
       "              ('hairy', 0.05555555555555555),\n",
       "              ('harry', 0.05555555555555555),\n",
       "              ('is', 0.05555555555555555),\n",
       "              ('jill', 0.05555555555555555),\n",
       "              ('and', 0.05555555555555555),\n",
       "              ('not', 0),\n",
       "              ('home', 0)]),\n",
       " OrderedDict([('to', 0),\n",
       "              (',', 0),\n",
       "              ('got', 0),\n",
       "              ('get', 0),\n",
       "              ('than', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('faster', 0),\n",
       "              ('store', 0),\n",
       "              ('as', 0.1111111111111111),\n",
       "              ('the', 0),\n",
       "              ('would', 0),\n",
       "              ('hairy', 0.05555555555555555),\n",
       "              ('harry', 0.05555555555555555),\n",
       "              ('is', 0.05555555555555555),\n",
       "              ('jill', 0.05555555555555555),\n",
       "              ('and', 0),\n",
       "              ('not', 0.05555555555555555),\n",
       "              ('home', 0)])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 余弦相似度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "document_vectors = []\n",
    "for doc in [doc_0, doc_1, doc_2]:\n",
    "\n",
    "    vec = copy.copy(vector_template)  # So we are dealing with new objects, not multiple references to the same object\n",
    "\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "\n",
    "    for key, value in token_counts.items():\n",
    "        vec[key] = value / len(lexicon)\n",
    "    document_vectors.append(vec)\n",
    "\n",
    "    \n",
    "import math\n",
    "\n",
    "def cosine_sim(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Since our vectors are dictionaries, lets convert them to lists for easier mathing.\n",
    "    \"\"\"\n",
    "    vec1 = [val for val in vec1.values()]\n",
    "    vec2 = [val for val in vec2.values()]\n",
    "    \n",
    "    dot_prod = 0\n",
    "    for i, v in enumerate(vec1):\n",
    "        dot_prod += v * vec2[i]\n",
    "        \n",
    "    mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "    \n",
    "    return dot_prod / (mag_1 * mag_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "余弦相似度为1表示两个向量相同。\n",
    "余弦相似度为0表示没有相同点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 齐普夫定律"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\69016\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "len(brown.words())  # words is a builtin method of the nltk corpus object that gives a list of tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 69971),\n",
       " ('of', 36412),\n",
       " ('and', 28853),\n",
       " ('to', 26158),\n",
       " ('a', 23195),\n",
       " ('in', 21337),\n",
       " ('that', 10594),\n",
       " ('is', 10109),\n",
       " ('was', 9815),\n",
       " ('he', 9548),\n",
       " ('for', 9489),\n",
       " ('it', 8760),\n",
       " ('with', 7289),\n",
       " ('as', 7253),\n",
       " ('his', 6996),\n",
       " ('on', 6741),\n",
       " ('be', 6377),\n",
       " ('at', 5372),\n",
       " ('by', 5306),\n",
       " ('i', 5164)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "puncs = [',', '.', '--', '-', '!', '?', ':', ';', '``', \"''\", '(', ')', '[', ']']\n",
    "word_list = [x.lower() for x in brown.words() if x not in puncs]\n",
    "token_counts = Counter(word_list)\n",
    "token_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主题建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlpia.data.loaders import kite_text, kite_history\n",
    "# intro_text = \"A kite is traditionally ...\"  # Step left to user, as above\n",
    "intro_text = kite_text.lower()\n",
    "intro_tokens = tokenizer.tokenize(intro_text)\n",
    "# history_text = \"Kites were invented in China, ...\"  # Also as above\n",
    "history_text = kite_history.lower()\n",
    "history_tokens = tokenizer.tokenize(history_text)\n",
    "intro_total = len(intro_tokens)\n",
    "history_total = len(history_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency of \"kite\" in intro is: 0.0440771349862259\n",
      "Term Frequency of \"kite\" in history is: 0.020202020202020204\n"
     ]
    }
   ],
   "source": [
    "intro_tf = {}\n",
    "history_tf = {}\n",
    "intro_counts = Counter(intro_tokens)\n",
    "intro_tf['kite'] = intro_counts['kite'] / intro_total\n",
    "history_counts = Counter(history_tokens)\n",
    "history_tf['kite'] = history_counts['kite'] / history_total\n",
    "print('Term Frequency of \"kite\" in intro is: {}'.format(intro_tf['kite']))\n",
    "print('Term Frequency of \"kite\" in history is: {}'.format(history_tf['kite']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kite的相关度 intro部分是history部分的两倍？\n",
    "在观察一下两篇文章中And的相关度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency of \"and\" in intro is: 0.027548209366391185\n",
      "Term Frequency of \"and\" in history is: 0.030303030303030304\n"
     ]
    }
   ],
   "source": [
    "intro_tf['and'] = intro_counts['and'] / intro_total\n",
    "history_tf['and'] = history_counts['and'] / history_total\n",
    "print('Term Frequency of \"and\" in intro is: {}'.format(intro_tf['and']))\n",
    "print('Term Frequency of \"and\" in history is: {}'.format(history_tf['and']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果一个词向在某篇文档中出现多次，但是却很少出现在语料库的其他文档中，那么就可以假设它对当前文档非常重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词项IDF\n",
    "词向IDF = 文档总数/出现 “某一次”的文档数\n",
    "两篇文章中的and/kite的IDF相同 但China不同 很明显China更加重要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs_containing_and = 0\n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'and' in doc:\n",
    "        num_docs_containing_and += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs_containing_kite = 0\n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'kite' in doc:\n",
    "        num_docs_containing_kite += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs_containing_china = 0\n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'china' in doc:\n",
    "        num_docs_containing_china += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_tf['china'] = intro_counts['china'] / intro_total\n",
    "history_tf['china'] = history_counts['china'] / history_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro_tf['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010101010101010102"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_tf['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = 2\n",
    "intro_idf = {}\n",
    "history_idf = {}\n",
    "intro_idf['and'] = num_docs / num_docs_containing_and \n",
    "history_idf['and'] = num_docs / num_docs_containing_and \n",
    "intro_idf['kite'] = num_docs / num_docs_containing_kite \n",
    "history_idf['kite'] = num_docs / num_docs_containing_kite \n",
    "intro_idf['china'] = num_docs / num_docs_containing_china \n",
    "history_idf['china'] = num_docs / num_docs_containing_china \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于语料库D中给定的文档d里的词频t有：\n",
    "tf(t,d) = t在d中出现的次数/d的长度\n",
    "idf(t,D) = lg(文档数/包含t的文档数)\n",
    "tfidf(t,d,D) = tf(t,d) * idf(t,D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF相关度排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('to', 0), (',', 0), ('got', 0), ('get', 0), ('than', 0), ('.', 0), ('faster', 0), ('store', 0), ('as', 0), ('the', 0), ('would', 0), ('hairy', 0), ('harry', 0), ('is', 0), ('jill', 0), ('and', 0), ('not', 0), ('home', 0)])\n",
      "OrderedDict([('to', 0), (',', 0), ('got', 0), ('get', 0), ('than', 0), ('.', 0), ('faster', 0), ('store', 0), ('as', 0), ('the', 0), ('would', 0), ('hairy', 0), ('harry', 0), ('is', 0), ('jill', 0), ('and', 0), ('not', 0), ('home', 0)])\n",
      "OrderedDict([('to', 0), (',', 0), ('got', 0), ('get', 0), ('than', 0), ('.', 0), ('faster', 0), ('store', 0), ('as', 0), ('the', 0), ('would', 0), ('hairy', 0), ('harry', 0), ('is', 0), ('jill', 0), ('and', 0), ('not', 0), ('home', 0)])\n"
     ]
    }
   ],
   "source": [
    "document_tfidf_vectors = []\n",
    "documents = [doc_0, doc_1, doc_2]\n",
    "for doc in documents:\n",
    "\n",
    "    vec = copy.copy(vector_template)  # So we are dealing with new objects, not multiple references to the same object\n",
    "    print(vec)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "\n",
    "    for key, value in token_counts.items():\n",
    "        docs_containing_key = 0\n",
    "        for _doc in documents:\n",
    "          if key in _doc:\n",
    "            docs_containing_key += 1\n",
    "        tf = value / len(lexicon)\n",
    "        if docs_containing_key:\n",
    "            idf = len(documents) / docs_containing_key\n",
    "        else:\n",
    "            idf = 0\n",
    "        vec[key] = tf * idf \n",
    "    document_tfidf_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('to', 0.16666666666666666),\n",
       "              (',', 0.5),\n",
       "              ('got', 0.16666666666666666),\n",
       "              ('get', 0.16666666666666666),\n",
       "              ('than', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('faster', 0.25),\n",
       "              ('store', 0.16666666666666666),\n",
       "              ('as', 0),\n",
       "              ('the', 0.6666666666666666),\n",
       "              ('would', 0.16666666666666666),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.0),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('and', 0),\n",
       "              ('not', 0),\n",
       "              ('home', 0.16666666666666666)]),\n",
       " OrderedDict([('to', 0),\n",
       "              (',', 0),\n",
       "              ('got', 0),\n",
       "              ('get', 0),\n",
       "              ('than', 0.16666666666666666),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('faster', 0.08333333333333333),\n",
       "              ('store', 0),\n",
       "              ('as', 0),\n",
       "              ('the', 0),\n",
       "              ('would', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('and', 0.16666666666666666),\n",
       "              ('not', 0),\n",
       "              ('home', 0)]),\n",
       " OrderedDict([('to', 0),\n",
       "              (',', 0),\n",
       "              ('got', 0),\n",
       "              ('get', 0),\n",
       "              ('than', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('faster', 0),\n",
       "              ('store', 0),\n",
       "              ('as', 0.1111111111111111),\n",
       "              ('the', 0),\n",
       "              ('would', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('and', 0),\n",
       "              ('not', 0.16666666666666666),\n",
       "              ('home', 0)])]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_tfidf_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用的语料库由3篇文章组成，查询的是“How long does it take to get the store?”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5235048549676834\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"How long does it take to get to the store?\"\n",
    "query_vec = copy.copy(vector_template) \n",
    "\n",
    "query_vec = copy.copy(vector_template)  # So we are dealing with new objects, not multiple references to the same object\n",
    "\n",
    "tokens = tokenizer.tokenize(query.lower())\n",
    "token_counts = Counter(tokens)\n",
    "\n",
    "for key, value in token_counts.items():\n",
    "    docs_containing_key = 0\n",
    "    for _doc in documents:\n",
    "      if key in _doc.lower():\n",
    "        docs_containing_key += 1\n",
    "    if docs_containing_key == 0:  # We didn't find that token in the lexicon go to next key\n",
    "        continue\n",
    "    tf = value / len(tokens)\n",
    "    idf = len(documents) / docs_containing_key \n",
    "    query_vec[key] = tf * idf \n",
    "\n",
    "print(cosine_sim(query_vec, document_tfidf_vectors[0]))\n",
    "print(cosine_sim(query_vec, document_tfidf_vectors[1]))\n",
    "print(cosine_sim(query_vec, document_tfidf_vectors[2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见doc0与查询句子相关性最高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建TF-IDF的工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.42662402 0.18698644 0.18698644 0.\n",
      "  0.22087441 0.18698644 0.         0.         0.         0.18698644\n",
      "  0.         0.74794576 0.18698644 0.18698644]\n",
      " [0.46312056 0.         0.35221512 0.         0.         0.35221512\n",
      "  0.27352646 0.         0.35221512 0.35221512 0.         0.\n",
      "  0.46312056 0.         0.         0.        ]\n",
      " [0.         0.75143242 0.         0.         0.         0.28574186\n",
      "  0.22190405 0.         0.28574186 0.28574186 0.37571621 0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [doc_0, doc_1, doc_2]\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "model = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(model.todense())  # The model becomes a sparse numpy matrix, as in a large corpus there would be mostly zeros to deal with.  todense() brings it back to a regular numpy matrix for our viewing pleasure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "齐普夫定律可以帮助我们预测各种事物的频率，包括词、字符和人物  \n",
    "在大多数NLP应用中，高维度向量之间的欧几里得距离和相似度不能充分表示他们之间的相似度\n",
    "余弦距离，即两个向量的重合度，可以将会议话的向量的元素相乘后再将乘积相加，从而实现高效的余弦计算\n",
    "余弦距离是大多数自然语言向量处理相似度的计算方法"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
